{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab397bd3bb4492296193f45321e648c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL, DDIMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "scheduler = DDIMScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n",
    "\n",
    "def text_enc(prompts):\n",
    "    inputs = tokenizer(prompts, padding=\"max_length\", max_length=77, return_tensors=\"pt\")\n",
    "    return text_encoder(inputs.input_ids.to(\"cuda\"))[0].to(torch.float16)\n",
    "\n",
    "def mk_samples_with_negative_prompt(prompts, neg_prompts, g=7.5, seed=100, steps=70):\n",
    "    bs = len(prompts)\n",
    "    text = text_enc(prompts)\n",
    "    neg_text = text_enc(neg_prompts)\n",
    "    emb = torch.cat([neg_text, text]).to(torch.float16)\n",
    "    if seed:\n",
    "        torch.manual_seed(seed)\n",
    "    height, width = 512, 512\n",
    "    latents = torch.randn((bs, unet.config.in_channels, height // 8, width // 8), dtype=torch.float16).to(\"cuda\")\n",
    "    scheduler.set_timesteps(steps)\n",
    "    latents = latents * scheduler.init_noise_sigma\n",
    "    for i, ts in enumerate(tqdm(scheduler.timesteps)):\n",
    "        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n",
    "        with torch.no_grad():\n",
    "            n, t = unet(inp, ts, encoder_hidden_states=emb.half()).sample.chunk(2)  # Ensure emb is half\n",
    "        pred = n + g * (t - n)\n",
    "        latents = scheduler.step(pred, ts, latents).prev_sample\n",
    "    with torch.no_grad():\n",
    "        return vae.decode(1 / 0.18215 * latents).sample\n",
    "\n",
    "prompts = [\"A little girl\"]\n",
    "neg_prompts = [\"A little boy\"]\n",
    "generated_images = mk_samples_with_negative_prompt(prompts, neg_prompts, g=7.5, seed=42, steps=70)\n",
    "\n",
    "from PIL import Image\n",
    "for i, img in enumerate(generated_images):\n",
    "    img = (img / 2 + 0.5).clamp(0, 1)\n",
    "    img = (img * 255).cpu().numpy().astype(\"uint8\").transpose(1, 2, 0)\n",
    "    Image.fromarray(img).save(f\"generated_image_{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
